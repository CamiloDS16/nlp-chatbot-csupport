{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Modeling: Chatbot 'Yoldi'\n",
    "\n",
    "This notebook is dedicated to the preprocessing and modeling phases for the development of the 'Yoldi' chatbot. It focuses on transforming the cleaned data into a format suitable for training machine learning models and developing the chatbot's response generation system.\n",
    "\n",
    "#### Preprocessing\n",
    "- **Objective**: Prepare and refine the data for model training and response system development.\n",
    "- **Steps Involved**:\n",
    "  - Application of the function to link customer queries to corresponding Customer Support responses in the cleaned dataset.\n",
    "  - Further preprocessing of the linked data, ensuring consistency and usability for model training.\n",
    "  - Extraction of features relevant to the chatbot's response system, such as topics, sentiment, and named entities.\n",
    "\n",
    "#### Intent Recognition and Response Generation\n",
    "- **Objective**: Develop mechanisms to recognize user intent and generate appropriate responses.\n",
    "- **Approach**:\n",
    "  - Explore various NLP techniques and algorithms for intent recognition without labeled data.\n",
    "  - Implement and evaluate different models for response generation, considering the context and intent of user queries.\n",
    "\n",
    "#### Model Training and Evaluation\n",
    "- **Objective**: Train and evaluate models for the chatbot's core functionalities.\n",
    "- **Methodology**:\n",
    "  - Train models for topic modeling, sentiment analysis, and intent recognition.\n",
    "  - Evaluate models using appropriate metrics to ensure effectiveness and accuracy.\n",
    "  - Fine-tune models based on evaluation results to improve performance.\n",
    "\n",
    "#### Response System Integration\n",
    "- **Objective**: Integrate trained models to create a coherent response system for the chatbot.\n",
    "- **Details**:\n",
    "  - Combine models to interpret user queries and generate relevant responses.\n",
    "  - Implement logic to handle various types of queries and maintain contextual relevance.\n",
    "\n",
    "#### Prototyping and Testing\n",
    "- **Objective**: Prototype the chatbot and conduct initial testing.\n",
    "- **Process**:\n",
    "  - Develop a basic User Interface for interacting with the chatbot.\n",
    "  - Conduct test runs to assess the chatbot's response accuracy and coherence.\n",
    "  - Gather feedback and insights for further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Cleaned Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:00,934 - INFO - Starting execution of load_data\n",
      "2023-12-09 11:46:03,822 - INFO - Data loading completed successfully\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/interim/cleaned_data.csv'\n",
    "df = load_data(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_text</th>\n",
       "      <th>query_inbound</th>\n",
       "      <th>query_response_tweet_id</th>\n",
       "      <th>query_in_response_to_tweet_id</th>\n",
       "      <th>response_id</th>\n",
       "      <th>response_text</th>\n",
       "      <th>response_inbound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>query_length</th>\n",
       "      <th>response_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1515</td>\n",
       "      <td>id like at least one month to go by where i do...</td>\n",
       "      <td>True</td>\n",
       "      <td>1514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1514</td>\n",
       "      <td>hello this does not sound good can you dm the ...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>Negative</td>\n",
       "      <td>122</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>425816</td>\n",
       "      <td>klasse vielen dank dafür könntet ihr dann bitt...</td>\n",
       "      <td>True</td>\n",
       "      <td>425815</td>\n",
       "      <td>425817.0</td>\n",
       "      <td>425815</td>\n",
       "      <td>hi tom weve escalated your case to our special...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>Negative</td>\n",
       "      <td>132</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>354377</td>\n",
       "      <td>will do thanks</td>\n",
       "      <td>True</td>\n",
       "      <td>354378</td>\n",
       "      <td>354376.0</td>\n",
       "      <td>354378</td>\n",
       "      <td>youre welcome steffi</td>\n",
       "      <td>False</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>Positive</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2231678</td>\n",
       "      <td>report a site published photoshop crack</td>\n",
       "      <td>True</td>\n",
       "      <td>2231677</td>\n",
       "      <td>2231679.0</td>\n",
       "      <td>2231677</td>\n",
       "      <td>hi bianko you can submit your report here let ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>39</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>810064</td>\n",
       "      <td>how is nairobi airport developing have you got...</td>\n",
       "      <td>True</td>\n",
       "      <td>810062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810062</td>\n",
       "      <td>you soon you can always check our lounge locat...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                         query_text  query_inbound   \n",
       "0      1515  id like at least one month to go by where i do...           True  \\\n",
       "1    425816  klasse vielen dank dafür könntet ihr dann bitt...           True   \n",
       "2    354377                                     will do thanks           True   \n",
       "3   2231678            report a site published photoshop crack           True   \n",
       "4    810064  how is nairobi airport developing have you got...           True   \n",
       "\n",
       "  query_response_tweet_id  query_in_response_to_tweet_id  response_id   \n",
       "0                    1514                            NaN         1514  \\\n",
       "1                  425815                       425817.0       425815   \n",
       "2                  354378                       354376.0       354378   \n",
       "3                 2231677                      2231679.0      2231677   \n",
       "4                  810062                            NaN       810062   \n",
       "\n",
       "                                       response_text  response_inbound   \n",
       "0  hello this does not sound good can you dm the ...             False  \\\n",
       "1  hi tom weve escalated your case to our special...             False   \n",
       "2                               youre welcome steffi             False   \n",
       "3  hi bianko you can submit your report here let ...             False   \n",
       "4  you soon you can always check our lounge locat...             False   \n",
       "\n",
       "   sentiment sentiment_class  query_length  response_length  \n",
       "0    -0.5423        Negative           122              113  \n",
       "1    -0.5994        Negative           132              136  \n",
       "2     0.4404        Positive            14               20  \n",
       "3     0.0000         Neutral            39               68  \n",
       "4     0.0000         Neutral            75               75  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:\n",
    "\n",
    "We will perform a series of steps to transform the data for modeling by doing this, we ensure the consistency on responses. We will also add some engineered features that will enhance the model for the response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:03,997 - INFO - Dataframe shape after dropping missing values: (494440, 12)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Drop rows where either query_text or response_text is missing\n",
    "    df.dropna(subset=['query_text', 'response_text'], inplace=True)\n",
    "    logging.info(f\"Dataframe shape after dropping missing values: {df.shape}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in handling missing values: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:04,173 - INFO - Reduced dataset size: 49444\n"
     ]
    }
   ],
   "source": [
    "# sampling query_id and corresponding response_id\n",
    "try:\n",
    "    subset_ids = df[['query_id', 'response_id']].drop_duplicates().sample(frac=0.1, random_state=42)\n",
    "# merging the sampled ids back with the original dataframe to get the full rows\n",
    "    df_sampled = pd.merge(subset_ids, df, on=['query_id', 'response_id'], how='inner')\n",
    "    df_model = df_sampled[['query_text', 'response_text']]\n",
    "    logging.info(f\"Reduced dataset size: {df_sampled.shape[0]}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error reducing the dataframe: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:09,753 - INFO - DataFrame saved successfully to ../data/processed/processed_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved at: ../data/processed/processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# saving preprocessed data to csv\n",
    "folder_path2 = '../data/'  # Adjust the path as needed\n",
    "file_name2 = 'processed/processed_data.csv'\n",
    "full_path2 = save_data(df, folder_path2, file_name2)\n",
    "\n",
    "if full_path2:\n",
    "    print(f\"DataFrame saved at: {full_path2}\")\n",
    "else:\n",
    "    print(\"Failed to save the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:10,164 - INFO - DataFrame saved successfully to ../data/processed/model_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved at: ../data/processed/processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# saving preprocessed data to csv\n",
    "folder_path3 = '../data/'  # Adjust the path as needed\n",
    "file_name3 = 'processed/model_data.csv'\n",
    "full_path3 = save_data(df_model, folder_path3, file_name3)\n",
    "\n",
    "if full_path2:\n",
    "    print(f\"DataFrame saved at: {full_path2}\")\n",
    "else:\n",
    "    print(\"Failed to save the DataFrame.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-based Seq2Seq Model Development for Chatbot 'Yoldi'\n",
    "\n",
    "This section of the notebook is dedicated to the development of a LSTM-based Seq2Seq model for the chatbot 'Yoldi', aimed at automating responses for customer queries. The process is divided into several key stages:\n",
    "\n",
    "#### Model Preparation\n",
    "- **Objective**: Setup the LSTM-based Seq2Seq model architecture.\n",
    "- **Tasks**:\n",
    "  - Define the Encoder and Decoder architecture.\n",
    "  - Set up embedding layers for text processing.\n",
    "  - Initialize LSTM layers and define model parameters.\n",
    "#### Data Splitting\n",
    "- **Objective**: Divide the dataset into training and testing subsets.\n",
    "- **Methodology**:\n",
    "  - Utilize the `train_test_split` method to segregate the data.\n",
    "  - Ensure a balanced representation of data in both training and testing sets.\n",
    "#### Model Training\n",
    "- **Objective**: Train the Seq2Seq model on the dataset.\n",
    "- **Approach**:\n",
    "  - Feed the training data into the model.\n",
    "  - Monitor performance metrics during the training process.\n",
    "  - Employ validation checks to assess model learning.\n",
    "#### Model Evaluation\n",
    "- **Objective**: Evaluate the trained model's performance.\n",
    "- **Techniques**:\n",
    "  - Apply the model to the test dataset.\n",
    "  - Analyze the model's accuracy, precision, recall, and other relevant metrics.\n",
    "  - Use techniques like confusion matrix, ROC curve, etc., for deeper analysis.\n",
    "#### Model Optimization\n",
    "- **Objective**: Fine-tune the model for improved performance.\n",
    "- **Strategies**:\n",
    "  - Adjust model hyperparameters like learning rate, batch size, etc.\n",
    "  - Experiment with different numbers of LSTM units and layers.\n",
    "  - Explore regularization techniques to prevent overfitting.\n",
    "#### Conclusion and Next Steps\n",
    "- Summarize the findings from the model development process into the project report draft.\n",
    "- Outline potential improvements, testing and deployment stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49444 entries, 0 to 49443\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   query_text     49444 non-null  object\n",
      " 1   response_text  49444 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 772.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:12,825 - INFO - Tokenization successful.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_texts(texts):\n",
    "    \"\"\"\"\n",
    "    Tokenize a list of texts and convert them into sequences.\n",
    "    Args:\n",
    "    texts (list of str): A list of texts to be tokenized.\n",
    "    Returns:\n",
    "    (Tokenizer, list): A tokenizer object and a list of text sequences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        return tokenizer, tokenizer.texts_to_sequences(texts)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during tokenization: {e}\")\n",
    "        raise\n",
    "\n",
    "# Tokenize texts\n",
    "try:\n",
    "    input_tokenizer, input_sequences = tokenize_texts(df_model['query_text'])\n",
    "    target_tokenizer, target_sequences = tokenize_texts(df_model['response_text'])\n",
    "    logging.info(\"Tokenization successful.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during tokenization: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:12,866 - INFO - Data split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Splitting the data into training and testing sets\n",
    "    train_input_texts, test_input_texts, train_target_texts, test_target_texts = train_test_split(\n",
    "        input_sequences, target_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "    logging.info(\"Data split into training and testing sets.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during data splitting: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Encoder and Decoder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:12,897 - INFO - Max encoder sequence length (train): 60\n",
      "2023-12-09 11:46:12,899 - INFO - Max decoder sequence length (train): 57\n",
      "2023-12-09 11:46:12,899 - INFO - Number of encoder tokens: 39684\n",
      "2023-12-09 11:46:12,899 - INFO - Number of decoder tokens: 22196\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # calculating maximum sequence lengths from the training data\n",
    "    max_encoder_seq_length = max(len(seq) for seq in train_input_texts)\n",
    "    max_decoder_seq_length = max(len(seq) for seq in train_target_texts)\n",
    "\n",
    "    # token counts\n",
    "    num_encoder_tokens = len(input_tokenizer.word_index) + 1  # +1 for padding token\n",
    "    num_decoder_tokens = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "    logging.info(f'Max encoder sequence length (train): {max_encoder_seq_length}')\n",
    "    logging.info(f'Max decoder sequence length (train): {max_decoder_seq_length}')\n",
    "    logging.info(f'Number of encoder tokens: {num_encoder_tokens}')\n",
    "    logging.info(f'Number of decoder tokens: {num_decoder_tokens}')\n",
    "except Exception as e:\n",
    "    logging.error(f'Error in calculating sequence lengths or number of tokens: {e}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_input_sequences, train_target_sequences, \n",
    "                 test_input_sequences, test_target_sequences, \n",
    "                 num_encoder_tokens, num_decoder_tokens, \n",
    "                 max_encoder_seq_length, max_decoder_seq_length):\n",
    "    \"\"\"\n",
    "    Prepare data for Seq2Seq model training and testing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pad sequences for training data\n",
    "        train_encoder_input_data = pad_sequences(train_input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "        train_decoder_input_data = pad_sequences(train_target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "        # Pad sequences for testing data\n",
    "        test_encoder_input_data = pad_sequences(test_input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "        test_decoder_input_data = pad_sequences(test_target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "        # Prepare decoder target data for training set\n",
    "        train_decoder_target_data = np.zeros((len(train_target_sequences), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        for i, seq in enumerate(train_target_sequences):\n",
    "            for t, token in enumerate(seq):\n",
    "                if t > 0:  # decoder_target_data will be ahead by one timestep and will not include the start token.\n",
    "                    train_decoder_target_data[i, t - 1, token] = 1.\n",
    "\n",
    "        # Prepare decoder target data for testing set\n",
    "        test_decoder_target_data = np.zeros((len(test_target_sequences), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        for i, seq in enumerate(test_target_sequences):\n",
    "            for t, token in enumerate(seq):\n",
    "                if t > 0:  # same logic as above\n",
    "                    test_decoder_target_data[i, t - 1, token] = 1.\n",
    "\n",
    "        logging.info(\"Data preparation for training and testing completed successfully.\")\n",
    "        return (train_encoder_input_data, train_decoder_input_data, train_decoder_target_data), (test_encoder_input_data, test_decoder_input_data, test_decoder_target_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data preparation: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:15,996 - INFO - Data preparation for training and testing completed successfully.\n",
      "2023-12-09 11:46:15,997 - INFO - Data for training and testing prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Prepare the training and testing data\n",
    "    (train_encoder_input_data, \n",
    "     train_decoder_input_data, \n",
    "     train_decoder_target_data), (test_encoder_input_data, \n",
    "                                  test_decoder_input_data, \n",
    "                                  test_decoder_target_data) = prepare_data(\n",
    "        train_input_texts, train_target_texts, \n",
    "        test_input_texts, test_target_texts, \n",
    "        num_encoder_tokens, num_decoder_tokens, \n",
    "        max_encoder_seq_length, max_decoder_seq_length)\n",
    "\n",
    "    logging.info(\"Data for training and testing prepared successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during preparing data for training and testing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build model\n",
    "def build_seq2seq_model(num_encoder_tokens, num_decoder_tokens, latent_dim=64):\n",
    "    \"\"\"\n",
    "    Build a Seq2Seq model.\n",
    "    Args:\n",
    "        num_encoder_tokens (int): Number of unique tokens in the input.\n",
    "        num_decoder_tokens (int): Number of unique tokens in the output.\n",
    "        latent_dim (int, optional): Dimensionality of the encoding space. Default to 256.\n",
    "    Returns:\n",
    "        Model: A Seq2Seq model instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "        encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "        _, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Decoder\n",
    "        decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "        decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Model\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        logging.info(\"Seq2Seq model built successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error building Seq2Seq model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:16,456 - INFO - Seq2Seq model built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 39684)]        0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None, 22196)]        0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 64),                 1017574   ['input_1[0][0]']             \n",
      "                              (None, 64),                 4                                       \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 64),           5698816   ['input_2[0][0]',             \n",
      "                              (None, 64),                            'lstm[0][1]',                \n",
      "                              (None, 64)]                            'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 22196)          1442740   ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17317300 (66.06 MB)\n",
      "Trainable params: 17317300 (66.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:16,471 - INFO - Seq2Seq model built and compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# Building and compiling the Seq2Seq model\n",
    "try:\n",
    "    # Build the model\n",
    "    seq2seq_model = build_seq2seq_model(num_encoder_tokens, num_decoder_tokens, latent_dim=64)\n",
    "\n",
    "    # Compile the model\n",
    "    seq2seq_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "    # Print model summary\n",
    "    seq2seq_model.summary()\n",
    "    logging.info(\"Seq2Seq model built and compiled successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during model building or compilation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    history = seq2seq_model.fit(\n",
    "        [train_encoder_input_data, train_decoder_input_data], \n",
    "        train_decoder_target_data,\n",
    "        batch_size=32,  # Adjust based on your system's capability\n",
    "        epochs=50,  # Number of epochs to train for\n",
    "        validation_data=([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "        verbose=1\n",
    "    )\n",
    "    logging.info(\"Model training completed successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during model training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model_path = '../models/seq2seq_chatbot.h5'  # Adjust path as needed\n",
    "save_model(seq2seq_model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "try:\n",
    "    test_loss = seq2seq_model.evaluate([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data)\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during model evaluation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
