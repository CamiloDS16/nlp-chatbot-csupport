{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Modeling: Chatbot 'Yoldi'\n",
    "\n",
    "This notebook is dedicated to the preprocessing and modeling phases for the development of the 'Yoldi' chatbot. It focuses on transforming the cleaned data into a format suitable for training machine learning models and developing the chatbot's response generation system.\n",
    "\n",
    "#### Preprocessing\n",
    "- **Objective**: Prepare and refine the data for model training and response system development.\n",
    "- **Steps Involved**:\n",
    "  - Application of the function to link customer queries to corresponding Customer Support responses in the cleaned dataset.\n",
    "  - Further preprocessing of the linked data, ensuring consistency and usability for model training.\n",
    "  - Extraction of features relevant to the chatbot's response system, such as topics, sentiment, and named entities.\n",
    "\n",
    "#### Intent Recognition and Response Generation\n",
    "- **Objective**: Develop mechanisms to recognize user intent and generate appropriate responses.\n",
    "- **Approach**:\n",
    "  - Explore various NLP techniques and algorithms for intent recognition without labeled data.\n",
    "  - Implement and evaluate different models for response generation, considering the context and intent of user queries.\n",
    "\n",
    "#### Model Training and Evaluation\n",
    "- **Objective**: Train and evaluate models for the chatbot's core functionalities.\n",
    "- **Methodology**:\n",
    "  - Train models for topic modeling, sentiment analysis, and intent recognition.\n",
    "  - Evaluate models using appropriate metrics to ensure effectiveness and accuracy.\n",
    "  - Fine-tune models based on evaluation results to improve performance.\n",
    "\n",
    "#### Response System Integration\n",
    "- **Objective**: Integrate trained models to create a coherent response system for the chatbot.\n",
    "- **Details**:\n",
    "  - Combine models to interpret user queries and generate relevant responses.\n",
    "  - Implement logic to handle various types of queries and maintain contextual relevance.\n",
    "\n",
    "#### Prototyping and Testing\n",
    "- **Objective**: Prototype the chatbot and conduct initial testing.\n",
    "- **Process**:\n",
    "  - Develop a basic User Interface for interacting with the chatbot.\n",
    "  - Conduct test runs to assess the chatbot's response accuracy and coherence.\n",
    "  - Gather feedback and insights for further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:28:55.171900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import necessary libraries for modeling\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Cleaned Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:40:32,880 - INFO - Starting execution of load_data\n",
      "2023-11-28 19:40:35,440 - INFO - Data loading completed successfully\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/interim/cleaned_data.csv'\n",
    "df = load_data(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>dep_parse</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2149594</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-09 00:55:53</td>\n",
       "      <td>@535047 Can you please confirm the requested d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2149595.0</td>\n",
       "      <td>can you please confirm the requested details i...</td>\n",
       "      <td>confirm request detail dm thank</td>\n",
       "      <td>['AUX', 'PRON', 'INTJ', 'VERB', 'DET', 'VERB',...</td>\n",
       "      <td>['aux', 'nsubj', 'intj', 'ROOT', 'det', 'amod'...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positive</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1561881</td>\n",
       "      <td>482478</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-11-04 04:57:53</td>\n",
       "      <td>What's with the pricing @GoDaddyHelp?\\n$12.96....</td>\n",
       "      <td>1561882,1561883,1561880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what s with the pricing ok t cs state ican ok ...</td>\n",
       "      <td>s pricing ok t cs state ican ok add option ok ...</td>\n",
       "      <td>['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'INTJ',...</td>\n",
       "      <td>['nsubj', 'csubj', 'prep', 'det', 'pobj', 'pre...</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positive</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956537</td>\n",
       "      <td>SpotifyCares</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-10-30 17:24:43</td>\n",
       "      <td>@580626 Hey Henry! It's an easter egg for the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1956538.0</td>\n",
       "      <td>hey henry it s an easter egg for the netflix s...</td>\n",
       "      <td>hey henry s easter egg netflix strange thing t...</td>\n",
       "      <td>['INTJ', 'INTJ', 'PRON', 'VERB', 'DET', 'ADJ',...</td>\n",
       "      <td>['intj', 'intj', 'nsubj', 'ROOT', 'det', 'amod...</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>[('henry s', 'PERSON'), ('netflix', 'GPE')]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1495379</td>\n",
       "      <td>AskCiti</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-05 00:32:31</td>\n",
       "      <td>@467204 Hello. We haven't heard from u. If u s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1495377.0</td>\n",
       "      <td>hello we haven t heard from u if u still requi...</td>\n",
       "      <td>hello haven t hear u u require assistance pls ...</td>\n",
       "      <td>['INTJ', 'PRON', 'VERB', 'PROPN', 'VERB', 'ADP...</td>\n",
       "      <td>['intj', 'nsubj', 'ROOT', 'nsubj', 'ccomp', 'p...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>582530</td>\n",
       "      <td>ATVIAssist</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-12-03 13:19:03</td>\n",
       "      <td>@257338 Apologies for the delay, please provid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>582531.0</td>\n",
       "      <td>apologies for the delay please provide us more...</td>\n",
       "      <td>apology delay provide detail include gamer tag...</td>\n",
       "      <td>['NOUN', 'ADP', 'DET', 'NOUN', 'INTJ', 'VERB',...</td>\n",
       "      <td>['nsubj', 'prep', 'det', 'pobj', 'intj', 'ROOT...</td>\n",
       "      <td>0.1531</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id     author_id  inbound           created_at   \n",
       "0   2149594         Tesco    False  2017-11-09 00:55:53  \\\n",
       "1   1561881        482478     True  2017-11-04 04:57:53   \n",
       "2   1956537  SpotifyCares    False  2017-10-30 17:24:43   \n",
       "3   1495379       AskCiti    False  2017-11-05 00:32:31   \n",
       "4    582530    ATVIAssist    False  2017-12-03 13:19:03   \n",
       "\n",
       "                                                text        response_tweet_id   \n",
       "0  @535047 Can you please confirm the requested d...                      NaN  \\\n",
       "1  What's with the pricing @GoDaddyHelp?\\n$12.96....  1561882,1561883,1561880   \n",
       "2  @580626 Hey Henry! It's an easter egg for the ...                      NaN   \n",
       "3  @467204 Hello. We haven't heard from u. If u s...                      NaN   \n",
       "4  @257338 Apologies for the delay, please provid...                      NaN   \n",
       "\n",
       "   in_response_to_tweet_id                                       cleaned_text   \n",
       "0                2149595.0  can you please confirm the requested details i...  \\\n",
       "1                      NaN  what s with the pricing ok t cs state ican ok ...   \n",
       "2                1956538.0  hey henry it s an easter egg for the netflix s...   \n",
       "3                1495377.0  hello we haven t heard from u if u still requi...   \n",
       "4                 582531.0  apologies for the delay please provide us more...   \n",
       "\n",
       "                                      processed_text   \n",
       "0                    confirm request detail dm thank  \\\n",
       "1  s pricing ok t cs state ican ok add option ok ...   \n",
       "2  hey henry s easter egg netflix strange thing t...   \n",
       "3  hello haven t hear u u require assistance pls ...   \n",
       "4  apology delay provide detail include gamer tag...   \n",
       "\n",
       "                                            pos_tags   \n",
       "0  ['AUX', 'PRON', 'INTJ', 'VERB', 'DET', 'VERB',...  \\\n",
       "1  ['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'INTJ',...   \n",
       "2  ['INTJ', 'INTJ', 'PRON', 'VERB', 'DET', 'ADJ',...   \n",
       "3  ['INTJ', 'PRON', 'VERB', 'PROPN', 'VERB', 'ADP...   \n",
       "4  ['NOUN', 'ADP', 'DET', 'NOUN', 'INTJ', 'VERB',...   \n",
       "\n",
       "                                           dep_parse  sentiment   \n",
       "0  ['aux', 'nsubj', 'intj', 'ROOT', 'det', 'amod'...     0.3612  \\\n",
       "1  ['nsubj', 'csubj', 'prep', 'det', 'pobj', 'pre...     0.6808   \n",
       "2  ['intj', 'intj', 'nsubj', 'ROOT', 'det', 'amod...    -0.3818   \n",
       "3  ['intj', 'nsubj', 'ROOT', 'nsubj', 'ccomp', 'p...     0.4215   \n",
       "4  ['nsubj', 'prep', 'det', 'pobj', 'intj', 'ROOT...     0.1531   \n",
       "\n",
       "                                      entities sentiment_class  topic  \n",
       "0                                           []        Positive    8.0  \n",
       "1                                           []        Positive    5.0  \n",
       "2  [('henry s', 'PERSON'), ('netflix', 'GPE')]        Negative    1.0  \n",
       "3                                           []        Positive    3.0  \n",
       "4                                           []         Neutral    3.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:\n",
    "\n",
    "We will perform a series of steps to transform the data for modeling by doing this, we ensure the consistency on responses. We will also add some engineered features that will enhance the model for the response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:40:46,093 - INFO - Queries and responses linked successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>response_processed_text</th>\n",
       "      <th>response_created_at</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>topic</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>dep_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>664563</td>\n",
       "      <td>278380</td>\n",
       "      <td>s movie yo</td>\n",
       "      <td>2017-11-21 00:59:51</td>\n",
       "      <td>able stream movie personal device flight check...</td>\n",
       "      <td>2017-11-21 01:18:25</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['SCONJ', 'VERB', 'DET', 'NOUN', 'PROPN']</td>\n",
       "      <td>['advmod', 'ROOT', 'det', 'nsubj', 'nsubj']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2647246</td>\n",
       "      <td>746880</td>\n",
       "      <td>account hack way time try fix refuse help get ...</td>\n",
       "      <td>2017-11-17 22:50:25</td>\n",
       "      <td>m sorry frustration receive e mail account spe...</td>\n",
       "      <td>2017-11-17 22:53:29</td>\n",
       "      <td>0.128</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['PRON', 'NOUN', 'AUX', 'VERB', 'ADV', 'ADV', ...</td>\n",
       "      <td>['poss', 'nsubjpass', 'auxpass', 'ccomp', 'adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2192747</td>\n",
       "      <td>641784</td>\n",
       "      <td>soon flight check bag</td>\n",
       "      <td>2017-11-09 19:22:43</td>\n",
       "      <td>min prior departure count bag late check kr</td>\n",
       "      <td>2017-11-09 19:27:54</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['SCONJ', 'ADV', 'ADP', 'DET', 'NOUN', 'AUX', ...</td>\n",
       "      <td>['advmod', 'advmod', 'prep', 'det', 'pobj', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1820428</td>\n",
       "      <td>546520</td>\n",
       "      <td>escalate issue delivery guy roll order show re...</td>\n",
       "      <td>2017-10-29 07:33:39</td>\n",
       "      <td>kindly provide detail ll look issue appropriat...</td>\n",
       "      <td>2017-10-29 07:47:00</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>[]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['INTJ', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN',...</td>\n",
       "      <td>['intj', 'ROOT', 'det', 'dobj', 'det', 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1067084</td>\n",
       "      <td>371817</td>\n",
       "      <td>dad accidentally account deactivate need activ...</td>\n",
       "      <td>2017-10-23 12:58:07</td>\n",
       "      <td>hi ve reply dm let s continue chat gu</td>\n",
       "      <td>2017-10-23 15:03:29</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>[]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['PRON', 'NOUN', 'ADV', 'VERB', 'PRON', 'NOUN'...</td>\n",
       "      <td>['poss', 'nsubj', 'advmod', 'ROOT', 'poss', 'n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id author_id                                     processed_text   \n",
       "17    664563    278380                                         s movie yo  \\\n",
       "23   2647246    746880  account hack way time try fix refuse help get ...   \n",
       "30   2192747    641784                              soon flight check bag   \n",
       "36   1820428    546520  escalate issue delivery guy roll order show re...   \n",
       "62   1067084    371817  dad accidentally account deactivate need activ...   \n",
       "\n",
       "             created_at                            response_processed_text   \n",
       "17  2017-11-21 00:59:51  able stream movie personal device flight check...  \\\n",
       "23  2017-11-17 22:50:25  m sorry frustration receive e mail account spe...   \n",
       "30  2017-11-09 19:22:43        min prior departure count bag late check kr   \n",
       "36  2017-10-29 07:33:39  kindly provide detail ll look issue appropriat...   \n",
       "62  2017-10-23 12:58:07              hi ve reply dm let s continue chat gu   \n",
       "\n",
       "    response_created_at  sentiment entities sentiment_class  topic   \n",
       "17  2017-11-21 01:18:25      0.000       []         Neutral    5.0  \\\n",
       "23  2017-11-17 22:53:29      0.128       []         Neutral    1.0   \n",
       "30  2017-11-09 19:27:54      0.000       []         Neutral    4.0   \n",
       "36  2017-10-29 07:47:00     -0.296       []        Negative    5.0   \n",
       "62  2017-10-23 15:03:29     -0.340       []        Negative    5.0   \n",
       "\n",
       "                                             pos_tags   \n",
       "17          ['SCONJ', 'VERB', 'DET', 'NOUN', 'PROPN']  \\\n",
       "23  ['PRON', 'NOUN', 'AUX', 'VERB', 'ADV', 'ADV', ...   \n",
       "30  ['SCONJ', 'ADV', 'ADP', 'DET', 'NOUN', 'AUX', ...   \n",
       "36  ['INTJ', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN',...   \n",
       "62  ['PRON', 'NOUN', 'ADV', 'VERB', 'PRON', 'NOUN'...   \n",
       "\n",
       "                                            dep_parse  \n",
       "17        ['advmod', 'ROOT', 'det', 'nsubj', 'nsubj']  \n",
       "23  ['poss', 'nsubjpass', 'auxpass', 'ccomp', 'adv...  \n",
       "30  ['advmod', 'advmod', 'prep', 'det', 'pobj', 'a...  \n",
       "36  ['intj', 'ROOT', 'det', 'dobj', 'det', 'compou...  \n",
       "62  ['poss', 'nsubj', 'advmod', 'ROOT', 'poss', 'n...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain necessary features\n",
    "features_to_keep = ['tweet_id', 'author_id', \n",
    "                    'processed_text', 'sentiment', 'entities', \n",
    "                    'sentiment_class', 'topic', 'pos_tags', 'dep_parse']\n",
    "# creating a separate df with important features\n",
    "df_features = df[features_to_keep]\n",
    "# linking queries and responses\n",
    "df_preprocessed = link_queries_responses(df)\n",
    "# merging back the retained features\n",
    "df_preprocessed = df_preprocessed.merge(df_features, on=['tweet_id', 'author_id', 'processed_text'], how='left')\n",
    "# dropping rows with NaN in response_processed_text\n",
    "df_preprocessed.dropna(subset=['response_processed_text'], inplace=True)\n",
    "\n",
    "# checking data quality\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:41:03,436 - INFO - Feature Engineering function applied successfully\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = feature_engineering(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>response_processed_text</th>\n",
       "      <th>response_created_at</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>topic</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>dep_parse</th>\n",
       "      <th>entity_count</th>\n",
       "      <th>text_length</th>\n",
       "      <th>unique_pos_count</th>\n",
       "      <th>sentence_complexity</th>\n",
       "      <th>vocab_diversity</th>\n",
       "      <th>product_entity_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>664563</td>\n",
       "      <td>278380</td>\n",
       "      <td>s movie yo</td>\n",
       "      <td>2017-11-21 00:59:51</td>\n",
       "      <td>able stream movie personal device flight check...</td>\n",
       "      <td>2017-11-21 01:18:25</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['SCONJ', 'VERB', 'DET', 'NOUN', 'PROPN']</td>\n",
       "      <td>['advmod', 'ROOT', 'det', 'nsubj', 'nsubj']</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2647246</td>\n",
       "      <td>746880</td>\n",
       "      <td>account hack way time try fix refuse help get ...</td>\n",
       "      <td>2017-11-17 22:50:25</td>\n",
       "      <td>m sorry frustration receive e mail account spe...</td>\n",
       "      <td>2017-11-17 22:53:29</td>\n",
       "      <td>0.128</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['PRON', 'NOUN', 'AUX', 'VERB', 'ADV', 'ADV', ...</td>\n",
       "      <td>['poss', 'nsubjpass', 'auxpass', 'ccomp', 'adv...</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2192747</td>\n",
       "      <td>641784</td>\n",
       "      <td>soon flight check bag</td>\n",
       "      <td>2017-11-09 19:22:43</td>\n",
       "      <td>min prior departure count bag late check kr</td>\n",
       "      <td>2017-11-09 19:27:54</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['SCONJ', 'ADV', 'ADP', 'DET', 'NOUN', 'AUX', ...</td>\n",
       "      <td>['advmod', 'advmod', 'prep', 'det', 'pobj', 'a...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1820428</td>\n",
       "      <td>546520</td>\n",
       "      <td>escalate issue delivery guy roll order show re...</td>\n",
       "      <td>2017-10-29 07:33:39</td>\n",
       "      <td>kindly provide detail ll look issue appropriat...</td>\n",
       "      <td>2017-10-29 07:47:00</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>[]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['INTJ', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN',...</td>\n",
       "      <td>['intj', 'ROOT', 'det', 'dobj', 'det', 'compou...</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1067084</td>\n",
       "      <td>371817</td>\n",
       "      <td>dad accidentally account deactivate need activ...</td>\n",
       "      <td>2017-10-23 12:58:07</td>\n",
       "      <td>hi ve reply dm let s continue chat gu</td>\n",
       "      <td>2017-10-23 15:03:29</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>[]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['PRON', 'NOUN', 'ADV', 'VERB', 'PRON', 'NOUN'...</td>\n",
       "      <td>['poss', 'nsubj', 'advmod', 'ROOT', 'poss', 'n...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id author_id                                     processed_text   \n",
       "17    664563    278380                                         s movie yo  \\\n",
       "23   2647246    746880  account hack way time try fix refuse help get ...   \n",
       "30   2192747    641784                              soon flight check bag   \n",
       "36   1820428    546520  escalate issue delivery guy roll order show re...   \n",
       "62   1067084    371817  dad accidentally account deactivate need activ...   \n",
       "\n",
       "             created_at                            response_processed_text   \n",
       "17  2017-11-21 00:59:51  able stream movie personal device flight check...  \\\n",
       "23  2017-11-17 22:50:25  m sorry frustration receive e mail account spe...   \n",
       "30  2017-11-09 19:22:43        min prior departure count bag late check kr   \n",
       "36  2017-10-29 07:33:39  kindly provide detail ll look issue appropriat...   \n",
       "62  2017-10-23 12:58:07              hi ve reply dm let s continue chat gu   \n",
       "\n",
       "    response_created_at  sentiment entities sentiment_class  topic   \n",
       "17  2017-11-21 01:18:25      0.000       []         Neutral    5.0  \\\n",
       "23  2017-11-17 22:53:29      0.128       []         Neutral    1.0   \n",
       "30  2017-11-09 19:27:54      0.000       []         Neutral    4.0   \n",
       "36  2017-10-29 07:47:00     -0.296       []        Negative    5.0   \n",
       "62  2017-10-23 15:03:29     -0.340       []        Negative    5.0   \n",
       "\n",
       "                                             pos_tags   \n",
       "17          ['SCONJ', 'VERB', 'DET', 'NOUN', 'PROPN']  \\\n",
       "23  ['PRON', 'NOUN', 'AUX', 'VERB', 'ADV', 'ADV', ...   \n",
       "30  ['SCONJ', 'ADV', 'ADP', 'DET', 'NOUN', 'AUX', ...   \n",
       "36  ['INTJ', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN',...   \n",
       "62  ['PRON', 'NOUN', 'ADV', 'VERB', 'PRON', 'NOUN'...   \n",
       "\n",
       "                                            dep_parse  entity_count   \n",
       "17        ['advmod', 'ROOT', 'det', 'nsubj', 'nsubj']             0  \\\n",
       "23  ['poss', 'nsubjpass', 'auxpass', 'ccomp', 'adv...             0   \n",
       "30  ['advmod', 'advmod', 'prep', 'det', 'pobj', 'a...             0   \n",
       "36  ['intj', 'ROOT', 'det', 'dobj', 'det', 'compou...             0   \n",
       "62  ['poss', 'nsubj', 'advmod', 'ROOT', 'poss', 'n...             0   \n",
       "\n",
       "    text_length  unique_pos_count  sentence_complexity  vocab_diversity   \n",
       "17           10                 0                    0              1.0  \\\n",
       "23           72                 0                    0              1.0   \n",
       "30           21                 0                    0              1.0   \n",
       "36           64                 0                    0              0.9   \n",
       "62           58                 0                    0              1.0   \n",
       "\n",
       "    product_entity_count  \n",
       "17                     0  \n",
       "23                     0  \n",
       "30                     0  \n",
       "36                     0  \n",
       "62                     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9824 entries, 17 to 136449\n",
      "Data columns (total 18 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   tweet_id                 9824 non-null   int64  \n",
      " 1   author_id                9824 non-null   object \n",
      " 2   processed_text           9592 non-null   object \n",
      " 3   created_at               9824 non-null   object \n",
      " 4   response_processed_text  9824 non-null   object \n",
      " 5   response_created_at      9824 non-null   object \n",
      " 6   sentiment                9824 non-null   float64\n",
      " 7   entities                 9824 non-null   object \n",
      " 8   sentiment_class          9824 non-null   object \n",
      " 9   topic                    9592 non-null   float64\n",
      " 10  pos_tags                 9824 non-null   object \n",
      " 11  dep_parse                9824 non-null   object \n",
      " 12  entity_count             9824 non-null   int64  \n",
      " 13  text_length              9824 non-null   int64  \n",
      " 14  unique_pos_count         9824 non-null   int64  \n",
      " 15  sentence_complexity      9824 non-null   int64  \n",
      " 16  vocab_diversity          9824 non-null   float64\n",
      " 17  product_entity_count     9824 non-null   int64  \n",
      "dtypes: float64(3), int64(6), object(9)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>entity_count</th>\n",
       "      <th>text_length</th>\n",
       "      <th>unique_pos_count</th>\n",
       "      <th>sentence_complexity</th>\n",
       "      <th>vocab_diversity</th>\n",
       "      <th>product_entity_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.824000e+03</td>\n",
       "      <td>9824.000000</td>\n",
       "      <td>9592.000000</td>\n",
       "      <td>9824.0</td>\n",
       "      <td>9824.000000</td>\n",
       "      <td>9824.0</td>\n",
       "      <td>9824.0</td>\n",
       "      <td>9824.000000</td>\n",
       "      <td>9824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.481305e+06</td>\n",
       "      <td>0.061170</td>\n",
       "      <td>4.502189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.779316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941806</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.693930e+05</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>2.263526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.491739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.300000e+01</td>\n",
       "      <td>-0.955200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.326430e+05</td>\n",
       "      <td>-0.102700</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.471785e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.238852e+06</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.987314e+06</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id    sentiment        topic  entity_count  text_length   \n",
       "count  9.824000e+03  9824.000000  9592.000000        9824.0  9824.000000  \\\n",
       "mean   1.481305e+06     0.061170     4.502189           0.0    50.779316   \n",
       "std    8.693930e+05     0.400826     2.263526           0.0    32.491739   \n",
       "min    6.300000e+01    -0.955200     0.000000           0.0     0.000000   \n",
       "25%    7.326430e+05    -0.102700     3.000000           0.0    27.000000   \n",
       "50%    1.471785e+06     0.000000     5.000000           0.0    48.000000   \n",
       "75%    2.238852e+06     0.361200     6.000000           0.0    68.000000   \n",
       "max    2.987314e+06     0.975300     9.000000           0.0   257.000000   \n",
       "\n",
       "       unique_pos_count  sentence_complexity  vocab_diversity   \n",
       "count            9824.0               9824.0      9824.000000  \\\n",
       "mean                0.0                  0.0         0.941806   \n",
       "std                 0.0                  0.0         0.161121   \n",
       "min                 0.0                  0.0         0.000000   \n",
       "25%                 0.0                  0.0         0.928571   \n",
       "50%                 0.0                  0.0         1.000000   \n",
       "75%                 0.0                  0.0         1.000000   \n",
       "max                 0.0                  0.0         1.000000   \n",
       "\n",
       "       product_entity_count  \n",
       "count                9824.0  \n",
       "mean                    0.0  \n",
       "std                     0.0  \n",
       "min                     0.0  \n",
       "25%                     0.0  \n",
       "50%                     0.0  \n",
       "75%                     0.0  \n",
       "max                     0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 11:51:25,014 - INFO - DataFrame saved successfully to ../data/processed/processed_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved at: ../data/processed/processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# saving preprocessed data to csv\n",
    "folder_path2 = '../data/'  # Adjust the path as needed\n",
    "file_name2 = 'processed/processed_data.csv'\n",
    "full_path2 = save_data(df_preprocessed, folder_path2, file_name2)\n",
    "\n",
    "if full_path2:\n",
    "    print(f\"DataFrame saved at: {full_path2}\")\n",
    "else:\n",
    "    print(\"Failed to save the DataFrame.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-based Seq2Seq Model Development for Chatbot 'Yoldi'\n",
    "\n",
    "This section of the notebook is dedicated to the development of a LSTM-based Seq2Seq model for the chatbot 'Yoldi', aimed at automating responses for customer queries. The process is divided into several key stages:\n",
    "\n",
    "#### Model Preparation\n",
    "- **Objective**: Setup the LSTM-based Seq2Seq model architecture.\n",
    "- **Tasks**:\n",
    "  - Define the Encoder and Decoder architecture.\n",
    "  - Set up embedding layers for text processing.\n",
    "  - Initialize LSTM layers and define model parameters.\n",
    "#### Data Splitting\n",
    "- **Objective**: Divide the dataset into training and testing subsets.\n",
    "- **Methodology**:\n",
    "  - Utilize the `train_test_split` method to segregate the data.\n",
    "  - Ensure a balanced representation of data in both training and testing sets.\n",
    "#### Model Training\n",
    "- **Objective**: Train the Seq2Seq model on the dataset.\n",
    "- **Approach**:\n",
    "  - Feed the training data into the model.\n",
    "  - Monitor performance metrics during the training process.\n",
    "  - Employ validation checks to assess model learning.\n",
    "#### Model Evaluation\n",
    "- **Objective**: Evaluate the trained model's performance.\n",
    "- **Techniques**:\n",
    "  - Apply the model to the test dataset.\n",
    "  - Analyze the model's accuracy, precision, recall, and other relevant metrics.\n",
    "  - Use techniques like confusion matrix, ROC curve, etc., for deeper analysis.\n",
    "#### Model Optimization\n",
    "- **Objective**: Fine-tune the model for improved performance.\n",
    "- **Strategies**:\n",
    "  - Adjust model hyperparameters like learning rate, batch size, etc.\n",
    "  - Experiment with different numbers of LSTM units and layers.\n",
    "  - Explore regularization techniques to prevent overfitting.\n",
    "#### Conclusion and Next Steps\n",
    "- Summarize the findings from the model development process into the project report draft.\n",
    "- Outline potential improvements, testing and deployment stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:32:41,542 - INFO - Starting execution of load_data\n",
      "2023-11-29 12:32:41,644 - INFO - Data loading completed successfully\n"
     ]
    }
   ],
   "source": [
    "file_path3 = '../data/processed/processed_data.csv'\n",
    "df_modeling = load_data(file_path=file_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def remove_nan_values(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where the specified column has NaN values.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to clean.\n",
    "    column_name (str): The name of the column to check for NaN values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with NaN values removed from the specified column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if column exists in DataFrame\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "\n",
    "        # Count NaN values before removal\n",
    "        nan_count_before = df[column_name].isna().sum()\n",
    "\n",
    "        # Remove NaN values\n",
    "        cleaned_df = df.dropna(subset=[column_name])\n",
    "\n",
    "        # Count NaN values after removal\n",
    "        nan_count_after = cleaned_df[column_name].isna().sum()\n",
    "\n",
    "        logging.info(f\"Removed {nan_count_before - nan_count_after} rows with NaN values from '{column_name}' column.\")\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in remove_nan_values function: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 16:44:11,721 - INFO - Dataset loaded successfully with 9824 pairs.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    input_texts = df_modeling['processed_text'].tolist()\n",
    "    target_texts = ['\\t' + text for text in df_modeling['response_processed_text'].tolist()]  # '\\t' as start token for responses\n",
    "\n",
    "    logging.info(\"Dataset loaded successfully with %d pairs.\", len(input_texts))\n",
    "except Exception as e:\n",
    "    logging.error(\"Error loading dataset: %s\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit tokenizer and return sequences and word index\n",
    "def tokenize_texts(texts):\n",
    "    \"\"\"\n",
    "    The function tokenizes the texts in the preprocessed_text.\n",
    "    Args:\n",
    "    Texts: array-like format of texts.\n",
    "    Returns:\n",
    "    str: The full path to the saved CSV file, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        word_index = tokenizer.word_index\n",
    "        return tokenizer, sequences, word_index\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during tokenization: %s\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the encoder and decoder data\n",
    "def prepare_data(input_texts, target_texts, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length):\n",
    "    try:\n",
    "        input_tokenizer, input_sequences, _ = tokenize_texts(input_texts)\n",
    "        target_tokenizer, target_sequences, _ = tokenize_texts(target_texts)\n",
    "        \n",
    "        encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "        decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "        # Initialize decoder_target_data\n",
    "        decoder_target_data = np.zeros((len(target_sequences), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        \n",
    "        for i, seq in enumerate(target_sequences):\n",
    "            for t, token in enumerate(seq):\n",
    "                if t > 0:  # decoder_target_data will be ahead by one timestep and will not include the start token.\n",
    "                    decoder_target_data[i, t - 1, token] = 1.\n",
    "        \n",
    "        logging.info(\"Data prepared successfully.\")\n",
    "        return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during data preparation: %s\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a Seq2Seq model\n",
    "def create_seq2seq_model(num_encoder_tokens, num_decoder_tokens, latent_dim=256):\n",
    "    try:\n",
    "        # Define encoder\n",
    "        encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "        encoder = LSTM(latent_dim, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Define decoder\n",
    "        decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "        decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Define the Seq2Seq model\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        logging.info(\"Seq2Seq model created successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during model creation: %s\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text float indices: [22, 30, 65, 67, 99, 122, 196, 197, 247, 276, 318, 378, 445, 504, 505, 516, 579, 581, 629, 834, 867, 880, 908, 947, 1022, 1181, 1197, 1232, 1287, 1392, 1415, 1417, 1424, 1524, 1525, 1554, 1585, 1678, 1688, 1707, 1710, 1727, 1778, 1790, 1822, 1895, 1900, 1911, 2014, 2015, 2067, 2070, 2072, 2164, 2187, 2265, 2328, 2334, 2347, 2437, 2472, 2496, 2568, 2580, 2616, 2668, 2686, 2751, 2808, 2819, 2840, 2848, 2913, 2932, 3091, 3108, 3109, 3117, 3181, 3201, 3202, 3285, 3386, 3428, 3477, 3538, 3559, 3642, 3693, 3727, 3751, 3767, 3856, 3871, 3890, 3953, 3979, 4017, 4029, 4054, 4070, 4076, 4100, 4329, 4599, 4606, 4618, 4715, 4774, 4786, 4851, 4901, 4955, 4997, 5018, 5031, 5061, 5065, 5071, 5188, 5214, 5224, 5239, 5283, 5290, 5297, 5331, 5441, 5552, 5569, 5638, 5692, 5695, 5717, 5805, 5835, 5854, 5868, 5872, 5937, 5957, 5971, 6026, 6073, 6183, 6195, 6223, 6240, 6270, 6278, 6299, 6300, 6349, 6388, 6463, 6495, 6586, 6590, 6599, 6613, 6672, 6714, 6735, 6751, 6865, 6934, 6963, 7022, 7024, 7063, 7157, 7287, 7333, 7355, 7356, 7441, 7443, 7460, 7535, 7586, 7618, 7624, 7637, 7675, 7680, 7696, 7742, 7869, 7903, 8062, 8101, 8130, 8156, 8167, 8239, 8294, 8296, 8300, 8306, 8312, 8381, 8532, 8610, 8635, 8868, 8972, 8998, 9000, 9036, 9040, 9081, 9089, 9135, 9155, 9168, 9171, 9243, 9360, 9392, 9409, 9416, 9467, 9506, 9507, 9564, 9577, 9613, 9635, 9732, 9802, 9808, 9820]\n",
      "Target text float indices: []\n"
     ]
    }
   ],
   "source": [
    "# Function to find indices of float values in lists\n",
    "def find_floats_in_list(text_list):\n",
    "    float_indices = [index for index, item in enumerate(text_list) if isinstance(item, float)]\n",
    "    return float_indices\n",
    "\n",
    "\n",
    "# Find indices of float values\n",
    "input_float_indices = find_floats_in_list(input_texts)\n",
    "target_float_indices = find_floats_in_list(target_texts)\n",
    "\n",
    "# Print out the indices\n",
    "print(\"Input text float indices:\", input_float_indices)\n",
    "print(\"Target text float indices:\", target_float_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 22, Value: nan\n",
      "Index: 30, Value: nan\n",
      "Index: 65, Value: nan\n",
      "Index: 67, Value: nan\n",
      "Index: 99, Value: nan\n",
      "Index: 122, Value: nan\n",
      "Index: 196, Value: nan\n",
      "Index: 197, Value: nan\n",
      "Index: 247, Value: nan\n",
      "Index: 276, Value: nan\n"
     ]
    }
   ],
   "source": [
    "# Print out examples of float values in the input texts\n",
    "for index in input_float_indices[:10]:  # Adjust the slice as needed\n",
    "    print(f\"Index: {index}, Value: {input_texts[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 16:59:53,299 - ERROR - Error calculating sequence lengths or number of tokens: object of type 'float' has no len()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Defining hyperparameters and sequence lengths\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     max_encoder_seq_length \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(\u001b[39mlen\u001b[39;49m(txt) \u001b[39mfor\u001b[39;49;00m txt \u001b[39min\u001b[39;49;00m input_texts \u001b[39mif\u001b[39;49;00m txt \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      4\u001b[0m     max_decoder_seq_length \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(txt) \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m target_texts \u001b[39mif\u001b[39;00m txt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     num_encoder_tokens \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mmax\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_sequences \u001b[39mif\u001b[39;00m seq) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# +1 for padding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Defining hyperparameters and sequence lengths\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     max_encoder_seq_length \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39;49m(txt) \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m input_texts \u001b[39mif\u001b[39;00m txt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     max_decoder_seq_length \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(txt) \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m target_texts \u001b[39mif\u001b[39;00m txt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     num_encoder_tokens \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mmax\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_sequences \u001b[39mif\u001b[39;00m seq) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# +1 for padding\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Defining hyperparameters and sequence lengths\n",
    "    max_encoder_seq_length = max(len(txt) for txt in input_texts if txt is not None)\n",
    "    max_decoder_seq_length = max(len(txt) for txt in target_texts if txt is not None)\n",
    "    num_encoder_tokens = max(max(seq) for seq in input_sequences if seq) + 1  # +1 for padding\n",
    "    num_decoder_tokens = max(max(seq) for seq in target_sequences if seq) + 1\n",
    "\n",
    "    logging.info(f'Max encoder sequence length: {max_encoder_seq_length}')\n",
    "    logging.info(f'Max decoder sequence length: {max_decoder_seq_length}')\n",
    "    logging.info(f'Number of encoder tokens: {num_encoder_tokens}')\n",
    "    logging.info(f'Number of decoder tokens: {num_decoder_tokens}')\n",
    "except Exception as e:\n",
    "    logging.error('Error calculating sequence lengths or number of tokens: %s', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "# Function to fit tokenizer and return sequences and word index\n",
    "def tokenize_texts(texts):\n",
    "    try:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        word_index = tokenizer.word_index\n",
    "        return tokenizer, sequences, word_index\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during tokenization: %s\", e)\n",
    "        raise\n",
    "\n",
    "# Function to prepare the encoder and decoder data\n",
    "def prepare_data(input_texts, target_texts, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length):\n",
    "    try:\n",
    "        input_tokenizer, input_sequences, _ = tokenize_texts(input_texts)\n",
    "        target_tokenizer, target_sequences, _ = tokenize_texts(target_texts)\n",
    "        \n",
    "        encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "        decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "        # Initialize decoder_target_data\n",
    "        decoder_target_data = np.zeros((len(target_sequences), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        \n",
    "        for i, seq in enumerate(target_sequences):\n",
    "            for t, token in enumerate(seq):\n",
    "                if t > 0:  # decoder_target_data will be ahead by one timestep and will not include the start token.\n",
    "                    decoder_target_data[i, t - 1, token] = 1.\n",
    "        \n",
    "        logging.info(\"Data prepared successfully.\")\n",
    "        return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during data preparation: %s\", e)\n",
    "        raise\n",
    "\n",
    "# Function to create a Seq2Seq model\n",
    "def create_seq2seq_model(num_encoder_tokens, num_decoder_tokens, latent_dim=256):\n",
    "    try:\n",
    "        # Define encoder\n",
    "        encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "        encoder = LSTM(latent_dim, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Define decoder\n",
    "        decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "        decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Define the Seq2Seq model\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        logging.info(\"Seq2Seq model created successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during model creation: %s\", e)\n",
    "        raise\n",
    "\n",
    "# Example of loading data from a file (assuming CSV format)\n",
    "# Here, we need to load your preprocessed dataset to fill `input_texts` and `target_texts`\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset_path = '/path/to/your/dataset.csv'  # Update with your dataset path\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Assuming 'processed_text' column is for input and 'response_processed_text' column is for target\n",
    "    input_texts = data['processed_text'].tolist()\n",
    "    target_texts = ['\\t' + text for text in data['response_processed_text'].tolist()]  # '\\t' as start token for responses\n",
    "\n",
    "    logging.info(\"Dataset loaded successfully with %d pairs.\", len(input_texts))\n",
    "except Exception as e:\n",
    "    logging.error(\"Error loading dataset: %s\", e)\n",
    "    raise\n",
    "\n",
    "# Continue with the rest of your code for data preparation and model training\n",
    "# ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
